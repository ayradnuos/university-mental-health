{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1E3CC3yykorn",
        "outputId": "e2875cb7-420f-451d-db9a-c4d86f9d8693"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPrahathish Kameswaran\\t\\t\\tRoopak Narayanasamy\\t\\t\\tSoundarya Venkatesh\\n         115062056 \\t\\t\\t         114941190 \\t\\t\\t       114711711\\n\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "GCP and Colab: This notebook has all the code for collecting and extracting the data from Reddit.\n",
        "This uses PySpark.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7QXTD6_RYCN",
        "outputId": "488cce53-688c-499a-af65-51da6960ecab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234911 sha256=540fbed989c4c25b46c1aee3bc3836b7e8e6b677a3aff638ef33932fab37a230\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ],
      "source": [
        "#  requirements to run this notebook\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                 # install java\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # download spark\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz                                                   # untar spark\n",
        "!pip install -q findspark                                                               # install findspark\n",
        "!pip install emoji                                                                      # install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTzX-BwYyVEX"
      },
      "outputs": [],
      "source": [
        "# setting up environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bT91MrtnRgbD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oacf3niRw6m"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time  \n",
        "\n",
        "# Get posts from a subreddit and Process them\n",
        "def processPosts(posts, sub, directory):\n",
        "    f = open(f'{directory}/{sub}-reddit.csv', \"a\", encoding=\"utf-8\")\n",
        "    for post in posts:\n",
        "      # add a delimiter \"DELIM\" to separate the features\n",
        "       s = str(post['created_utc']) + \"DELIM\" + \" \".join(post['title'].splitlines()) + \"DELIM\" + \" \".join(post['selftext'].splitlines())  + \"DELIM\" + str(post['score']) + \"\\n\"\n",
        "       f.write(s)\n",
        "    f.close()\n",
        "    return posts[-1]['created_utc']\n",
        "\n",
        "# Get URL for a subreddit\n",
        "def getURL(sub, before):\n",
        "    url = \"https://api.pushshift.io/reddit/submission/search/?size=1000&sort=created_utc&subreddit=\" + sub\n",
        "    if before:\n",
        "        url += \"&before=\"+str(before)\n",
        "    return url\n",
        "\n",
        "# Fetch posts from a list of subreddits \n",
        "def fetchRedditPosts(subreddits, directory):\n",
        "    for sub in subreddits:\n",
        "        beforeTime = int(time.time())\n",
        "        print(f\"Fetching posts for {sub} before: \", beforeTime)\n",
        "        while True:\n",
        "            response = requests.get(getURL(sub, beforeTime))\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                # The request was successful\n",
        "                posts = response.json()['data']\n",
        "\n",
        "                if len(posts) == 0:\n",
        "                    break\n",
        "                beforeTime = processPosts(posts, sub, directory)\n",
        "                print(f\"\\nNow will fetch {sub} posts before: \", beforeTime)\n",
        "            elif response.status_code == 429:\n",
        "                print(\"Ratelimited. Trying again in a minute..\")\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                # The request was unsuccessful\n",
        "                print('Request failed with status code:', response.status_code)\n",
        "                #Try again\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvHaUTyeS53O"
      },
      "outputs": [],
      "source": [
        "# call another py file to combine all the files into one\n",
        "from utils import combineFiles\n",
        "\n",
        "# Get Mental Health data\n",
        "MHdirectory = \"Data/MH\"\n",
        "\n",
        "# List of Mental Health subreddits\n",
        "MHsubreddits = [\"Stress\", \"Anxiety\", \"depression\", \"AnxietyDepression\", \"mentalhealth\", \"bipolar\", \"SuicideWatch\", \"MentalHealthSupport\", \"mentalillness\", \"socialanxiety\", \"Anxietyhelp\", \"PanicAttack\", \"Anger\", \"rant\"]\n",
        "for sub in MHsubreddits:\n",
        "\n",
        "    # call the function to fetch posts from a subreddit\n",
        "    fetchRedditPosts(sub, MHdirectory)\n",
        "    \n",
        "# save all the files into one\n",
        "MHdatapath = os.path.join(MHdirectory, \"mental-health.csv\")\n",
        "files = [sub+\"-reddit.csv\" for sub in MHsubreddits]\n",
        "combineFiles(MHdirectory, files, MHdatapath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaBISbgOU8Nh"
      },
      "outputs": [],
      "source": [
        "with open(\"univ-reddit-handles\", \"r\") as fi:\n",
        "    univ_reddits = fi.read().split(\",\")\n",
        "\n",
        "for univ in univ_reddits:\n",
        "    fetchRedditPosts(sub, \"Data/University\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-tT4e2rY_L8"
      },
      "outputs": [],
      "source": [
        "# Get NON-Mental Health data from huggingface \n",
        "!wget https://huggingface.co/datasets/sentence-transformers/reddit-title-body/resolve/main/reddit_title_text_2021.jsonl.gz\n",
        "!gunzip reddit_title_text_2021.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9JvH4ZAZnC7"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy.linalg import pinv\n",
        "import findspark\n",
        "import sys\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import AccumulatorParam\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# create spark session\n",
        "spark = SparkSession.builder \\\n",
        "\t\t.master(\"local[*]\") \\\n",
        "\t\t.appName(\"Project\") \\\n",
        "\t\t.getOrCreate()  \n",
        "\n",
        "# read the data\n",
        "data = spark.sparkContext.textFile(\"reddit_title_text_2021.jsonl\")\n",
        "print(\"Total no. of lines\", data.count())\n",
        "\n",
        "# load the data\n",
        "data = data.map(json.loads)\n",
        "\n",
        "# filter out the posts from the mental health subreddits\n",
        "print(\"\\n\\nCheckpoint: \", data.count())\n",
        "sub_broadcast = spark.sparkContext.broadcast(MHsubreddits + univ_reddits)\n",
        "\n",
        "def filterFunc(rec):\n",
        "    MHsubs = sub_broadcast.value\n",
        "    return rec[\"subreddit\"] not in MHsubs\n",
        "\n",
        "def mapFunc(post):\n",
        "    # We are using placeholder Timestamp and Scores because it does not matter for training.\n",
        "    # We are adding them to keep it uniform with the university reddit data.\n",
        "    placeholderTS = \"1683942542\"\n",
        "    placeholderScore = \"1\"\n",
        "    return placeholderTS + \"DELIM\" + \" \".join(post['title'].splitlines()) + \"DELIM\" + \" \".join(post['body'].splitlines())  + \"DELIM\" + placeholderScore\n",
        "\n",
        "data = data.filter(filterFunc).map(mapFunc)\n",
        "data.saveAsTextFile(\"Data/Non-MH\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFXOiAKsao0L",
        "outputId": "816d9b5a-beae-4a87-ea6c-67cd3acc5187"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# call another py file to preprocess the data\n",
        "from utils import preprocessData\n",
        "\n",
        "preMHdir = \"/Data/MH/Preprocessed\"\n",
        "# Get preprocessed Mental Health data\n",
        "preprocessData(\"/Data/MH/mental-health.csv\", preMHdir)\n",
        "\n",
        "preNMHdir = \"/Data/Non-MH/Preprocessed\"\n",
        "# Get preprocessed Non-Mental Health data\n",
        "preprocessData(\"/Data/Non-MH\", preNMHdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxJUqF5ph79K"
      },
      "outputs": [],
      "source": [
        "# call combineParts to combine all the files into one\n",
        "def combineParts(directory, output_file):\n",
        "    # List files in the directory\n",
        "    files = os.listdir(directory)\n",
        "\n",
        "    # Filter files with the name pattern \"part-<5 digit number>\"\n",
        "    filtered_files = [file for file in files if file.startswith(\"part-\")]\n",
        "    combineFiles(directory, filtered_files, output_file)\n",
        "\n",
        "# Combine all the parts into one\n",
        "combineParts(preMHdir, \"Model/train_MH\")\n",
        "combineParts(preNMHdir, \"Model/train_NMH\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
