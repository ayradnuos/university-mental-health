{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "863noK4DkpM8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GCP and Colab:This notebook has all the code for analysis of the sentiment scores and performing temporal analysis, topic modeling and word cloud.\n",
        "This uses PySpark for processing the scores.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sLeAIOgvFoi"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                     # install java\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz     # download spark\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz                                                       # untar spark                   \n",
        "!pip install -q findspark                                                                   # install findspark                                       \n",
        "!pip install emoji                                                                          # install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXvTsJdxvORS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"         # set java home        \n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"       # set spark home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tREm5x3GvRBC"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdjomzSIwXSF"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# import emoji library\n",
        "def LDA(preprocessed_posts):\n",
        "    print(f\"Preprocessed {len(preprocessed_posts)} records..\")\n",
        "\n",
        "    # Create a document-term matrix using CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "\n",
        "    # doc_term_matrix is a sparse matrix\n",
        "    doc_term_matrix = vectorizer.fit_transform(preprocessed_posts) \n",
        "\n",
        "    print(\"LDA Checkpoint 1\")\n",
        "    # Define the number of topics to discover\n",
        "    num_topics = 10\n",
        "\n",
        "    # Apply LDA topic modeling\n",
        "    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42) \n",
        "    lda_model.fit(doc_term_matrix) \n",
        "\n",
        "    print(\"LDA Checkpoint 2\")\n",
        "    # Extract the most common topics\n",
        "    feature_names = vectorizer.get_feature_names_out() \n",
        "    most_common_topics = []\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):   \n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]    \n",
        "        most_common_topics.append((topic_idx, top_words))   \n",
        "\n",
        "    # Print the most common topics\n",
        "    for topic in most_common_topics:\n",
        "        print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnoOw33svSm2"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import datetime\n",
        "from pyspark import AccumulatorParam\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import json\n",
        "\n",
        "# create spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()  \n",
        "\n",
        "# class to accumulate dictionaries\n",
        "class DictParam(AccumulatorParam):\n",
        "    def zero(self, value = dict()):\n",
        "        return value\n",
        "\t\n",
        "    def addInPlace(self, dict1 ,dict2):\n",
        "        for k, v in dict2.items():\n",
        "            if k in dict1:\n",
        "                dict1[k] = dict1[k] + v\n",
        "            else:\n",
        "                dict1[k] = v\n",
        "        return dict1\n",
        "\n",
        "# to store the accumulated values\n",
        "month_acc = spark.sparkContext.accumulator(dict(), DictParam()) #Divide by total length\n",
        "count_acc = spark.sparkContext.accumulator(dict(), DictParam()) \n",
        "word_acc = spark.sparkContext.accumulator(dict(), DictParam())\n",
        "\n",
        "# map and aggregate\n",
        "def mapAndAggregate(rec):\n",
        "    global month_acc\n",
        "    global count_acc\n",
        "    rec = rec.split(\"DELIM\")\n",
        "    try:\n",
        "        date = datetime.datetime.fromtimestamp(float(rec[1]))\n",
        "        k = date.month#.strftime(\"%Y-%m\")\n",
        "        month_acc += { k: float(rec[3]) }\n",
        "        count_acc += { k: 1 }\n",
        "    except:\n",
        "        print(\"ERROR\", rec)\n",
        "    return rec\n",
        "\n",
        "# read data\n",
        "data = spark.sparkContext.textFile(\"Data/University-Scores/combined-scores\").map(mapAndAggregate) \n",
        "length = data.count()\n",
        "\n",
        "acad_year = month_acc.value \n",
        "post_count = count_acc.value    \n",
        "with open(\"month-acc.json\", \"w\") as fi: \n",
        "    json.dump(acad_year, fi)\n",
        "with open(\"count-acc.json\", \"w\") as fi:\n",
        "    json.dump(post_count, fi)\n",
        "\n",
        "for month in month_acc.value.keys():\n",
        "    acad_year[month] /= post_count[month]\n",
        "\n",
        "acad_year = sorted(acad_year.items())\n",
        "months, values = zip(*acad_year)\n",
        "\n",
        "# Plot the monthly values\n",
        "plt.plot(months, values)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Monthly Values')\n",
        "plt.xticks(range(1, 13))  # Set x-axis ticks for each month\n",
        "# plt.xticks(range(2008, 2024))  # Set x-axis ticks for each month\n",
        "plt.show()\n",
        "\n",
        "print(\"Checkpoint1: \", length)\n",
        "stressData = data.filter(lambda x: float(x[3]) >= 0.7)\n",
        "print(\"Checkpoint2, Stress data: \", stressData.count())\n",
        "\n",
        "# Topic modeling\n",
        "TMdata = stressData.map(lambda x: x[2]).collect()\n",
        "print(\"Collected stress data, calling LDA\")\n",
        "LDA(TMdata)\n",
        "\n",
        "# Generate word frequencies\n",
        "def collect_word_freq(rec):\n",
        "    global word_acc\n",
        "    temp = {}\n",
        "    for word in rec[2].split():\n",
        "        if word in temp:\n",
        "            temp[word] += 1\n",
        "        else:\n",
        "            temp[word] = 1\n",
        "    word_acc += temp\n",
        "\n",
        "# WordCloud\n",
        "stressData.foreach(collect_word_freq)\n",
        "print(\"Word frequencies done\")\n",
        "word_freq = word_acc.value\n",
        "with open(\"word-acc.json\", \"w\") as fi:\n",
        "    json.dump(word_freq, fi)\n",
        "\n",
        "# # Create the word cloud\n",
        "# wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "# # Display the word cloud using matplotlib\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud, interpolation='bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "# stop spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6ZQOhWWwRsl"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the English stop words from NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk_stop_words = stopwords.words('english')\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "# Get the English stop words from the package\n",
        "py_stop_words = get_stop_words('en')\n",
        "\n",
        "# Define custom stop words\n",
        "custom_stop_words = [\"anyone\", \"get\", \"really\", \"im\", \"going\", \"sure\", \"y’all\", \"also\", \"keep\", \"one\", \"like\", \"else\", \"i’m\", \"it’s\", \"got\" , \"say\", \"go\" , \"come\", \"dont\", \"don’t\", \"take\", \"day\", \"thought\", \"two\", \"getting\", \"you’re\", \"since\", \"even\", \"cant\", \"taken\", \"know\", \"someone\", \"i’ve\", \"ive\", \"well\", \"making\", \"thing\", \"didn’t\", \"haven’t\", \"want\", \"yet\", \"wanted\", \"make\", \"already\", \"coming\"]\n",
        "\n",
        "# Combine the lists of stop words\n",
        "combined = set(nltk_stop_words + py_stop_words + custom_stop_words)\n",
        "\n",
        "with open(\"word-acc.json\", \"r\") as fi:\n",
        "    word_freq = json.load(fi)\n",
        "\n",
        "keys_to_delete = [key for key in word_freq.keys() if key in combined or len(key) <= 2]  # Delete stop words and words with length <= 2\n",
        "\n",
        "for key in keys_to_delete:\n",
        "    del word_freq[key]\n",
        "\n",
        "# Create the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC9Bf4eafJ_m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fig 3: Analysis of stress scores of college Reddit communities over the past 10 years\n",
        "\"\"\"\n",
        "# Create area chart using Plotly\n",
        "import json\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "with open('/content/month-acc.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "# Convert the data dictionary to a DataFrame\n",
        "df = pd.DataFrame.from_dict(data, orient='index', columns=['Value'])\n",
        "df = df.sort_index()\n",
        "# Reset index and rename columns\n",
        "df = df.reset_index().rename(columns={'index': 'Year'})\n",
        "df['Year'] = df['Year'].astype(int)\n",
        "\n",
        "highlight_mask = (df['Year'] >= 2020) & (df['Year'] <= 2022)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Scatter(x=df['Year'], \n",
        "               y=df['Value'], \n",
        "               fill='tozeroy', \n",
        "               mode='none',\n",
        "              #  fillcolor='rgb(204, 204, 225)',\n",
        "               fillcolor='rgb(36, 142, 124)',\n",
        "               opacity=0.01\n",
        "               )\n",
        "])\n",
        "\n",
        "\n",
        "# Highlight the years 2019-2022 as \"covid\" with red color\n",
        "fig.update_traces(\n",
        "    marker=dict(color='red'),\n",
        "    # selector=dict(x=df['Year'].between(2019, 2022))\n",
        "    selector = dict({'Year':2010})\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Trend Analysis',\n",
        "    xaxis=dict(\n",
        "        title='Year',\n",
        "        tickmode='array',\n",
        "        tickvals=df['Year'],\n",
        "        ticktext=df['Year']\n",
        "    ),\n",
        "    yaxis=dict(title='Value'),\n",
        "    showlegend=False\n",
        ")\n",
        "fig.update_layout(yaxis_range=[0.2,0.5])\n",
        "fig.update_layout(xaxis_range=[2012,2022])\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df['Year'][highlight_mask],\n",
        "    y=df['Value'][highlight_mask],\n",
        "    mode='lines',\n",
        "    fill='tozeroy',\n",
        "    fillpattern=dict(fgcolor='rgb(153, 153, 255)', fillmode='replace', shape=\"x\"),\n",
        "    # fillcolor='rgb(255, 255, 153)',\n",
        "    # fillcolor='rgb(153, 153, 225)',\n",
        "    line=dict(color='rgb(153, 153, 255)'),\n",
        "    name='Covid Years',\n",
        "    opacity=.5\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Analysis of stress scores of college reddit communities over the past 10 years',\n",
        "    yaxis=dict(title='Average Stress scores'),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Clu9ikxHfwqg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load the data from University Stress Scores CSV file into a Pandas DataFrame\n",
        "data = pd.read_csv('/content/university_stress_scores.csv', names=['subreddit','scores'])\n",
        "\n",
        "# data.drop(data[data['subreddit'] == 'colum'].index, inplace = True)\n",
        "data['scores'] = data['scores'] - data['scores'].mean()\n",
        "# Extract the university subreddit names and stress scores from the DataFrame\n",
        "university_subreddits = data['subreddit']\n",
        "stress_scores = data['scores']\n",
        "\n",
        "# Create a color scale using an increasing color gradient\n",
        "color_scale = px.colors.sequential.Reds\n",
        "\n",
        "# Create the bar graph using Plotly\n",
        "fig = go.Figure(data=[go.Bar(x=university_subreddits, \n",
        "                             y=stress_scores\n",
        "                             )])\n",
        "\n",
        "fig.update_layout(xaxis=dict(\n",
        "                  title='Xaxis Name',\n",
        "                  tickmode='linear'),\n",
        "                  xaxis_title='University Subreddit', \n",
        "                  yaxis_title='Deviation from Avg. Stress Scores')\n",
        "\n",
        "# Set the color scale for the bar graph\n",
        "fig.update_traces(marker=dict(colorscale=color_scale))\n",
        "# Set the x-axis label orientation\n",
        "fig.update_layout(xaxis_tickangle=-45)\n",
        "# Set category order for x-axis to \"category ascending\"\n",
        "fig.update_xaxes(categoryorder='category ascending')\n",
        "\n",
        "# Set the category array to display all categories on the x-axis\n",
        "fig.update_xaxes(categoryarray=university_subreddits)\n",
        "# fig.update_layout(xaxis_range=[0,0.5])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4oG3PEaf-Nw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fig 4: Temporal analysis over an academic year\n",
        "\"\"\"\n",
        "import plotly.express as px\n",
        "import calendar\n",
        "\n",
        "# Load the data from JSON file into a pandas DataFrame\n",
        "data = pd.read_json('/content/acad-year-acc.json', orient='index')\n",
        "data.reset_index(inplace=True)\n",
        "data.columns = ['month', 'score']\n",
        "data = data.sort_values('month')\n",
        "\n",
        "data['month'] = data['month'].apply(lambda x: calendar.month_abbr[x])\n",
        "# Sort the data by month in ascending order\n",
        "# Create the line chart using Plotly\n",
        "fig = px.line(data, x='month', y='score')\n",
        "\n",
        "# Customize the chart layout\n",
        "fig.update_layout(xaxis_title='Months',\n",
        "                  yaxis_title='Average Stess Score')\n",
        "\n",
        "# Show the chart\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toigvWRtgnqb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/month-acc.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        "# Convert the data dictionary to a DataFrame\n",
        "df = pd.DataFrame.from_dict(data, orient='index', columns=['Value'])\n",
        "df = df.sort_index()\n",
        "# Reset index and rename columns\n",
        "df = df.reset_index().rename(columns={'index': 'Year'})\n",
        "df['Year'] = df['Year'].astype(int)\n",
        "\n",
        "highlight_mask = (df['Year'] >= 2020) & (df['Year'] <= 2022)\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Scatter(x=df['Year'], \n",
        "               y=df['Value'], \n",
        "               fill='tozeroy', \n",
        "               mode='none',\n",
        "               fillcolor='rgb(36, 142, 124)',\n",
        "               opacity=0.01\n",
        "               )\n",
        "])\n",
        "\n",
        "\n",
        "# Highlight the years 2019-2022 as \"covid\"\n",
        "fig.update_traces(\n",
        "    marker=dict(color='red'),\n",
        "    # selector=dict(x=df['Year'].between(2019, 2022))\n",
        "    selector = dict({'Year':2010})\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Trend Analysis',\n",
        "    xaxis=dict(\n",
        "        title='Year',\n",
        "        tickmode='array',\n",
        "        tickvals=df['Year'],\n",
        "        ticktext=df['Year']\n",
        "    ),\n",
        "    yaxis=dict(title='Value'),\n",
        "    showlegend=False\n",
        ")\n",
        "fig.update_layout(yaxis_range=[0.2,0.5])\n",
        "fig.update_layout(xaxis_range=[2012,2022])\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=df['Year'][highlight_mask],\n",
        "    y=df['Value'][highlight_mask],\n",
        "    mode='lines',\n",
        "    fill='tozeroy',\n",
        "    fillpattern=dict(fgcolor='rgb(153, 153, 255)', fillmode='replace', shape=\"x\"),\n",
        "    line=dict(color='rgb(153, 153, 255)'),\n",
        "    name='Covid Years',\n",
        "    opacity=.5\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Analysis of stress scores of college reddit communities over the past 10 years',\n",
        "    yaxis=dict(title='Average Stress scores'),\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
