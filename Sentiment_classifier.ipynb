{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiXnL4y0k_2S"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GCP and Colab: This notebook has all the code for building the sentiment classifier, training and using the model to obtain the scores.\n",
        "This uses PyTorch.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVu2GGilQ4I0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IePuEkT6mwWo",
        "outputId": "f8f810af-6fc7-454c-dabf-408fc78ce919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234911 sha256=27e1bf35f48e9d3d7f6da8372a7517e96dad9f13adc27912eff73357133725fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "#  install all the requirements\n",
        "!pip install transformers\n",
        "!pip install emoji\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85SfiJsFnotx"
      },
      "outputs": [],
      "source": [
        "MH_path = \"Model/train_MH\"\n",
        "NMH_path = \"Model/train_NMH\"\n",
        "\n",
        "# Open the file in read mode\n",
        "with open(MH_path, 'r', encoding=\"utf-8\") as file:\n",
        "    # Read the lines of the file and store them in a list\n",
        "    MHtext = file.readlines()\n",
        "\n",
        "# Print the lines\n",
        "print(len(MHtext))\n",
        "MHlabels = [1]*len(MHtext)\n",
        "\n",
        "# Open the file in read mode\n",
        "with open(NMH_path, 'r', encoding=\"utf-8\") as file:\n",
        "    # Read the lines of the file and store them in a list\n",
        "    NMHtext = file.readlines()\n",
        "\n",
        "# Print the lines\n",
        "print(len(NMHtext))\n",
        "NMHlabels = [0]*len(NMHtext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHYWumL-n8al"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts = MHtext + NMHtext\n",
        "labels = MHlabels + NMHlabels\n",
        "# Split data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train data: {len(train_texts)}\")\n",
        "print(f\"Test data: {len(test_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKzFQS4goQXB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Binary Sentiment classifier\n",
        "\"\"\"\n",
        "\n",
        "# Import the libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Define the Sentiment Analysis Model\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        \n",
        "        # Freeze the BERT model's parameters\n",
        "        for param in bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.bert = bert_model              # DistilBert\n",
        "        self.dropout = nn.Dropout(0.1)      # Dropout layer to prevent overfitting\n",
        "        self.linear = nn.Linear(768, 1)     # linear layer\n",
        "        # self.linear = nn.Linear(512, 1)   #MobileBert\n",
        "        self.sigmoid = nn.Sigmoid()         # Sigmoid activation function\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)     # Feed input to BERT\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]                          # Use the [CLS] token representation\n",
        "        dropout_output = self.dropout(pooled_output)                                # Pass the pooled output to the dropout layer\n",
        "        logits = self.linear(dropout_output)                                        # Pass the dropout output to the linear layer\n",
        "        probabilities = self.sigmoid(logits)                                        # Pass the logits to the sigmoid activation function\n",
        "        return probabilities\n",
        "\n",
        "# Custom Dataset for Sentiment Analysis\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts              # Input texts\n",
        "        self.labels = labels            # Labels\n",
        "        self.tokenizer = tokenizer      # Tokenizer for encoding the text\n",
        "        self.max_len = max_len          # Maximum length of the tokenized input\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    # Create a tokenized input and its corresponding label\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.texts[index])\n",
        "        label = self.labels[index]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,                           # Text to be tokenized\n",
        "            add_special_tokens=True,        # Add special tokens\n",
        "            max_length=self.max_len,        # Maximum length of the tokenized input\n",
        "            padding='max_length',           # Pad the sequences to the maximum length\n",
        "            truncation=True,                # Truncate the input to the maximum length\n",
        "            return_attention_mask=True,     # Return the attention mask\n",
        "            return_tensors='pt'             # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64         # Number of samples in each batch\n",
        "max_length = 512        # Max length of the text that can go to BERT\n",
        "learning_rate = 16e-4   # Learning rate\n",
        "num_epochs = 1          # Number of epochs\n",
        "\n",
        "# Prepare the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')  # Load the DistilBERT tokenizer\n",
        "\n",
        "# Initialize the model\n",
        "model = SentimentClassifier()   # Sentiment classifier\n",
        "model.to(device)                # Push the model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPSZPnBqojYx"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, max_length)  # Training dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   # Training dataloader\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.linear.parameters(), lr=learning_rate)   # Adam optimizer\n",
        "criterion = nn.BCELoss()    # Cross entropy loss\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.5)  # Scheduler for the learning rate\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()   # Set the model to training mode\n",
        "    total_loss = 0  # Total loss\n",
        "    intermediate_loss = 0   # Intermediate loss\n",
        "    i = 0\n",
        "    for batch in train_loader:\n",
        "        # print(f\"Batch {i}\")\n",
        "        i += 1\n",
        "        input_ids = batch['input_ids'].to(device)               # Push the batch to GPU\n",
        "        attention_mask = batch['attention_mask'].to(device)     # Push the batch to GPU\n",
        "        labels = batch['label'].to(device)                      # Push the batch to GPU\n",
        "\n",
        "        optimizer.zero_grad()                               # Clear the previous gradients\n",
        "        probabilities = model(input_ids, attention_mask)    # Feed the input to the model\n",
        "        loss = criterion(probabilities.squeeze(), labels)   # Calculate the loss\n",
        "\n",
        "        loss.backward()     # Backpropagate the loss\n",
        "        optimizer.step()    # Update the weights\n",
        "\n",
        "        loss_item = loss.item()             # Get the loss value from the loss tensor\n",
        "        total_loss += loss_item             # Add the loss for this batch to the total loss\n",
        "        intermediate_loss += loss_item      # Add the loss for this batch to the intermediate loss\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(f\"Batch average loss: {intermediate_loss/200} | Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
        "            intermediate_loss = 0\n",
        "            if i%500 == 0:\n",
        "                scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)                                   # Calculate the average loss\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')      # Print the average loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUoywGwyomgy"
      },
      "outputs": [],
      "source": [
        "# - test_texts: List of test texts\n",
        "# - test_labels: List of corresponding test labels (0 or 1)\n",
        "# - tokenizer: Tokenizer instance for text encoding\n",
        "# - max_length: Maximum sequence length for encoding\n",
        "\n",
        "# Create the test dataset\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, tokenizer, max_length)     # Test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)        # Test dataloader\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()        # Set the model to evaluation mode\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)               # Push the batch to GPU\n",
        "        attention_mask = batch['attention_mask'].to(device)     # Push the batch to GPU\n",
        "        labels = batch['label'].to(device)                      # Push the batch to GPU\n",
        "\n",
        "        probabilities = model(input_ids, attention_mask)                        # Feed the input to the model\n",
        "        predicted_labels = (probabilities > 0.5).squeeze().cpu().numpy()        # Get the predicted labels\n",
        "\n",
        "        predictions.extend(predicted_labels)        # Append the predicted labels to a list\n",
        "        true_labels.extend(labels.cpu().numpy())    # Append the true labels to a list\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)         # Calculate the accuracy\n",
        "precision = precision_score(true_labels, predictions)       # Calculate the precision\n",
        "recall = recall_score(true_labels, predictions)             # Calculate the recall\n",
        "f1 = f1_score(true_labels, predictions)                     # Calculate the F1-score\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMYw7SgpowVe"
      },
      "outputs": [],
      "source": [
        "# Specify the file path where you want to save the weights\n",
        "weights_path = \"Model/model_weights.pth\"\n",
        "\n",
        "# Save the model's state_dict (containing the weights)\n",
        "torch.save(model.state_dict(), weights_path)\n",
        "\n",
        "print(\"Model weights saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkXdqvjDo0PR"
      },
      "outputs": [],
      "source": [
        "# Download the saved model to your local machine\n",
        "from google.colab import files\n",
        "files.download(weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFjsExVFo-ob"
      },
      "outputs": [],
      "source": [
        "# Load the weights into the model\n",
        "model.load_state_dict(torch.load(weights_path))\n",
        "\n",
        "# Set the model in evaluation mode\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnWdPLs4r4_W"
      },
      "outputs": [],
      "source": [
        "# import libraries and modules\n",
        "import os\n",
        "from utils import preprocessRec\n",
        "\n",
        "# Function to evaluate records from files\n",
        "def evaluate_records_from_universities(files):\n",
        "    for file in files:\n",
        "        sum = 0\n",
        "        count = 0\n",
        "        file_path = os.path.join(directory, file)                           # File path\n",
        "        uni = file[:-10]                                                    # University name\n",
        "        output_file_path = os.path.join(out_directory, uni+\"scores\")        # Output file path\n",
        "        print(f\"Evaluating {file}, writing into {output_file_path}..\")      # Print the file being evaluated\n",
        "        fo = open(output_file_path, 'w')\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                # Preprocess the line if needed\n",
        "                rec = preprocessRec(line, True)     # Preprocess the record\n",
        "                text = rec[1]                       # Get the text\n",
        "\n",
        "                # Encode the text\n",
        "                inputs = tokenizer.encode_plus(\n",
        "                    text,                           # Sentence to encode.\n",
        "                    add_special_tokens=True,        # Add '[CLS]' and '[SEP]'\n",
        "                    max_length=max_length,          # Pad & truncate all sentences.\n",
        "                    padding='max_length',           # Pad all sentences.\n",
        "                    truncation=True,                # Truncate all sentences.\n",
        "                    return_attention_mask=True,     # Construct attention masks.\n",
        "                    return_tensors='pt'             # Return pytorch tensors.\n",
        "                )\n",
        "\n",
        "                # Push the input to GPU\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "\n",
        "                # Push the attention mask to GPU\n",
        "                attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "                # Forward pass through the model\n",
        "                with torch.no_grad():\n",
        "                    logit = model(input_ids, attention_mask).to('cpu').item()       # Get the logit\n",
        "                sum += logit                                                        # Add the logit to the sum\n",
        "                count += 1                                                          # Increment the count\n",
        "                fo.write(f\"{uni}DELIM{rec[0]}DELIM{rec[1]}DELIM{logit}\\n\")          # Write the record to the output file\n",
        "        print(f\"University: {uni}, Score: {sum/count}\\n\")                           # Print the average score\n",
        "        fo.close()\n",
        "\n",
        "\n",
        "directory = \"Data/University/\"\n",
        "out_directory = \"Data/University-Scores/\"\n",
        "\n",
        "# List of files in the directory\n",
        "files = os.listdir(directory)                   \n",
        "\n",
        "# Evaluate records from files\n",
        "evaluate_records_from_universities(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbg--R5XtzRx"
      },
      "outputs": [],
      "source": [
        "# Import the combineFiles module\n",
        "import combineFiles                             \n",
        "\n",
        "score_dir = \"Data/University-Scores/\"                                           # Directory containing the scores\n",
        "files = os.listdir(score_dir)                                                   # List of files in the directory\n",
        "combineFiles(score_dir, files, \"Data/University-Scores/combined-scores\")        # Combine the scores into a single file"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
